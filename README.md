# mLLM ğŸ”¥

ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨ C++ ç¼–å†™ï¼Œä¸“æ³¨äºå­¦ä¹ å’Œç†è§£ Transformer æ¶æ„çš„åº•å±‚å®ç°ï¼

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ğŸ¤– æ¨¡å‹æ”¯æŒ**: ç›®å‰æ”¯æŒ Qwen3-0.6B æ¨¡å‹ï¼Œç›´æ¥å…¼å®¹ Hugging Face å®˜æ–¹æ¨¡å‹æ–‡ä»¶æ ¼å¼
- **âš¡ åŒåç«¯æ”¯æŒ**: å®ç°äº† CUDA å’Œ CPU ç®—å­ï¼Œæ”¯æŒ GPU å’Œ CPU æ¨ç†
- **ğŸ§® Sgemm**: å®ç°äº† CUDA çš„ Sgemm ç®—å­ï¼Œç›®å‰æ€§èƒ½è¾¾åˆ° cublas çš„ 82%
- **ğŸ“ BPE Tokenizer**: è‡ªä¸»å®ç°çš„å­—èŠ‚å¯¹ç¼–ç åˆ†è¯å™¨
- **ğŸ’¾ KV Cache**: å®ç°äº†é”®å€¼ç¼“å­˜æœºåˆ¶ï¼Œæå‡æ¨ç†æ•ˆç‡

## ğŸ›ï¸ é¡¹ç›®ç»“æ„

```
MiniLLM/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ base/                  # åŸºç¡€ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ allocator.h        # å†…å­˜åˆ†é…å™¨
â”‚   â”‚   â”œâ”€â”€ buffer.h           # æ•°æ®ç¼“å†²åŒº
â”‚   â”‚   â”œâ”€â”€ common.h           # é€šç”¨å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ json.hpp           # JSON è§£æåº“
â”‚   â”‚   â”œâ”€â”€ safetensors.h      # SafeTensors æ–‡ä»¶è§£æ
â”‚   â”‚   â”œâ”€â”€ tensor.h           # å¼ é‡ç±»å®šä¹‰
â”‚   â”‚   â””â”€â”€ util.h             # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ kernel/                # è®¡ç®—å†…æ ¸
â”‚   â”‚   â”œâ”€â”€ cpu/               # CPU ç®—å­å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ cuda/              # CUDA ç®—å­å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ kernel.h           # å†…æ ¸æ¥å£
â”‚   â”œâ”€â”€ model/                 # æ¨¡å‹æ¶æ„
â”‚   â”‚   â”œâ”€â”€ qwen3.h            # Qwen3 ä¸»æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ qwen3_decode_layer.h    # è§£ç å±‚
â”‚   â”‚   â”œâ”€â”€ qwen3_mlp.h        # å¤šå±‚æ„ŸçŸ¥æœº
â”‚   â”‚   â”œâ”€â”€ qwen3_rotary_embedding.h # æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â””â”€â”€ qwen3_self_attn.h  # è‡ªæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ op/                    # ç®—å­å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ add.h              # åŠ æ³•æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ causal_mask.h      # å› æœæ©ç 
â”‚   â”‚   â”œâ”€â”€ ele_mul.h          # å…ƒç´ ä¹˜æ³•
â”‚   â”‚   â”œâ”€â”€ embedding.h        # åµŒå…¥å±‚
â”‚   â”‚   â”œâ”€â”€ layer.h            # å±‚åŸºç±»
â”‚   â”‚   â”œâ”€â”€ linear.h           # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ mat_mul.h          # çŸ©é˜µä¹˜æ³•
â”‚   â”‚   â”œâ”€â”€ rms_norm.h         # RMS å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ silu.h             # SiLU æ¿€æ´»å‡½æ•°
â”‚   â”‚   â””â”€â”€ softmax.h          # Softmax æ“ä½œ
â”‚   â””â”€â”€ tokenizer/             # åˆ†è¯å™¨
â”‚       â””â”€â”€ tokenizer.h        # åˆ†è¯å™¨æ¥å£
â”œâ”€â”€ src/                       # æºæ–‡ä»¶å®ç°
â”‚   â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ kernel/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ op/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ qwen3_chat.cpp         # Qwen3 èŠå¤©æ¼”ç¤º
â”‚   â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ test/                      # æµ‹è¯•ç¨‹åº
â”œâ”€â”€ scripts/                   # è¾…åŠ©è„šæœ¬
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ README.md
```

## ğŸ§® Sgemm æ€§èƒ½å¯¹æ¯”
**ä½¿ç”¨è®¾å¤‡çš„æ˜¯ 4090**

M=N=K, M æ•´é™¤64çš„æƒ…å†µ
![SgemmDiv64æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_div64.png)

M=N=K, M æ•´é™¤1çš„æƒ…å†µ<br>
è¿™ç§æƒ…å†µç›®å‰åªæ˜¯åœ¨ div64 çš„åŸºç¡€ä¸Šå°†æ‰€æœ‰ float4 å­˜å–å±•å¼€ï¼Œå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œåç»­ä¼˜åŒ–TODO
![SgemmDiv1æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_div1.png)


## ğŸ¤– Qwen3 æ¶æ„å›¾
![Qwen3æ¶æ„å›¾](https://github.com/BowTen/mLLM/raw/main/resources/qwen3_arc.png)

## âŒ¨ï¸ èŠå¤©ç¤ºä¾‹

```cpp
#include "model/qwen3.h"

using namespace mllm;
using namespace mllm::tokenizer;

class Qwen3Chat
{
public:
    Qwen3Chat(const std::string &model_path, mllm::base::Device device, float temperature)
        : model(mllm::model::Qwen3::from_pretrained(model_path, device, temperature)),
          tokenizer(model.get_tokenizer())
    {
    }

    std::string chat(const std::string &input)
    {
        auto chat_token_ids = tokenizer->encode_with_chat_template(input, true, true);
        auto input_id = tokenizer->to_tensor(chat_token_ids, model.device());
        base::Tensor next_id({1, 1}, model.device(), false, model.stream());

        std::string output;
        while (true)
        {
            input_id.toDevice(model.device());
            next_id.toDevice(model.device());
            model.forward(input_id, next_id);
            next_id.toDevice(base::Device::CPU);
            uint32_t id = *(reinterpret_cast<uint32_t *>(next_id.data()));

            std::string next_token = tokenizer->decode(id);
            output.append(next_token);

            if (id == BPETokenizer::QWEN3_END_OF_TEXT || id == BPETokenizer::QWEN3_IM_END)
            {
                std::cout << std::endl;
                break;
            }
            std::cout << next_token;
            std::cout.flush();
            input_id = next_id.clone();
        }

        return output;
    }

private:
    mllm::model::Qwen3 model;
    BPETokenizerPtr tokenizer;
};

int main()
{
    std::string model_path = "path_to_your_resources/Qwen/Qwen3-0.6B";
    std::cout << "Loading model..." << std::endl;
    Qwen3Chat qwen3(model_path, base::Device::CUDA, 0.6);
    std::cout << "Loading accomplished." << std::endl;

    while (true)
    {
        std::string input;
        std::cout << "User: ";
        std::cin >> input;
        if (input == "exit")
            break;
        std::cout << "Qwen3: ";
        std::cout.flush();
        qwen3.chat(input);
    }

    return 0;
}
```

![Qwen3 chat demo æ¼”ç¤ºåŠ¨å›¾](https://github.com/BowTen/mLLM/raw/main/resources/chat_demo.gif)


## ä¾èµ–åº“

- armadillo
- glog
- gtest
# mLLM ðŸ”¥

ä¸€ä¸ªä»Žé›¶å¼€å§‹å®žçŽ°çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†æ¡†æž¶ï¼Œä½¿ç”¨ C++ ç¼–å†™ï¼Œä¸“æ³¨äºŽå­¦ä¹ å’Œç†è§£ Transformer æž¶æž„çš„åº•å±‚å®žçŽ°ï¼

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **ðŸ¤– æ¨¡åž‹æ”¯æŒ**: ç›®å‰æ”¯æŒ Qwen3-0.6B æ¨¡åž‹ï¼Œç›´æŽ¥å…¼å®¹ Hugging Face å®˜æ–¹æ¨¡åž‹æ–‡ä»¶æ ¼å¼
- **âš¡ åŒåŽç«¯æ”¯æŒ**: å®žçŽ°äº† CUDA å’Œ CPU ç®—å­ï¼Œæ”¯æŒ GPU å’Œ CPU æŽ¨ç†
- **ðŸ§® Sgemm**: å®žçŽ°äº† CUDA çš„ Sgemm ç®—å­ï¼Œåœ¨ RTX3060 ä¸Šæ€§èƒ½è¾¾åˆ° cublas çš„ 105%ï¼ŒRTX4090 ä¸Šè¾¾åˆ° 82%
- **ðŸ“ BPE Tokenizer**: è‡ªä¸»å®žçŽ°çš„å­—èŠ‚å¯¹ç¼–ç åˆ†è¯å™¨
- **ðŸ’¾ KV Cache**: å®žçŽ°äº†é”®å€¼ç¼“å­˜æœºåˆ¶ï¼Œæå‡æŽ¨ç†æ•ˆçŽ‡

## ðŸ›ï¸ é¡¹ç›®ç»“æž„

```
MiniLLM/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ base/                  # åŸºç¡€ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ allocator.h        # å†…å­˜åˆ†é…å™¨
â”‚   â”‚   â”œâ”€â”€ buffer.h           # æ•°æ®ç¼“å†²åŒº
â”‚   â”‚   â”œâ”€â”€ common.h           # é€šç”¨å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ json.hpp           # JSON è§£æžåº“
â”‚   â”‚   â”œâ”€â”€ safetensors.h      # SafeTensors æ–‡ä»¶è§£æž
â”‚   â”‚   â”œâ”€â”€ tensor.h           # å¼ é‡ç±»å®šä¹‰
â”‚   â”‚   â””â”€â”€ util.h             # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ kernel/                # è®¡ç®—å†…æ ¸
â”‚   â”‚   â”œâ”€â”€ cpu/               # CPU ç®—å­å®žçŽ°
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ cuda/              # CUDA ç®—å­å®žçŽ°
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ kernel.h           # å†…æ ¸æŽ¥å£
â”‚   â”œâ”€â”€ model/                 # æ¨¡åž‹æž¶æž„
â”‚   â”‚   â”œâ”€â”€ qwen3.h            # Qwen3 ä¸»æ¨¡åž‹
â”‚   â”‚   â”œâ”€â”€ qwen3_decode_layer.h    # è§£ç å±‚
â”‚   â”‚   â”œâ”€â”€ qwen3_mlp.h        # å¤šå±‚æ„ŸçŸ¥æœº
â”‚   â”‚   â”œâ”€â”€ qwen3_rotary_embedding.h # æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â””â”€â”€ qwen3_self_attn.h  # è‡ªæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ op/                    # ç®—å­å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ add.h              # åŠ æ³•æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ causal_mask.h      # å› æžœæŽ©ç 
â”‚   â”‚   â”œâ”€â”€ ele_mul.h          # å…ƒç´ ä¹˜æ³•
â”‚   â”‚   â”œâ”€â”€ embedding.h        # åµŒå…¥å±‚
â”‚   â”‚   â”œâ”€â”€ layer.h            # å±‚åŸºç±»
â”‚   â”‚   â”œâ”€â”€ linear.h           # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ mat_mul.h          # çŸ©é˜µä¹˜æ³•
â”‚   â”‚   â”œâ”€â”€ rms_norm.h         # RMS å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ silu.h             # SiLU æ¿€æ´»å‡½æ•°
â”‚   â”‚   â””â”€â”€ softmax.h          # Softmax æ“ä½œ
â”‚   â””â”€â”€ tokenizer/             # åˆ†è¯å™¨
â”‚       â””â”€â”€ tokenizer.h        # åˆ†è¯å™¨æŽ¥å£
â”œâ”€â”€ src/                       # æºæ–‡ä»¶å®žçŽ°
â”‚   â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ kernel/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ op/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ qwen3_chat.cpp         # Qwen3 èŠå¤©æ¼”ç¤º
â”‚   â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ test/                      # æµ‹è¯•ç¨‹åº
â”œâ”€â”€ scripts/                   # è¾…åŠ©è„šæœ¬
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ README.md
```

## ðŸ§® Sgemm ä¼˜åŒ–
### ä¸»è¦ä¼˜åŒ–æ‰‹æ®µï¼š
- **çŸ©é˜µåˆ†å—**: å°†çŸ©é˜µåˆ†ä¸ºè‹¥å¹²tileï¼Œæ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£ä¸€ä¸ªtileçš„è®¡ç®—ã€‚æ¯ä¸ªtileå†åˆ†ä¸ºè‹¥å¹²fragï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€ä¸ªfragçš„è®¡ç®—ã€‚çº¿ç¨‹é—´å¹¶è¡Œè®¡ç®—fragï¼Œæé«˜æ•ˆçŽ‡ã€‚åŒæ—¶å¤§é‡çº¿ç¨‹å¹¶å‘å¯éšè—è®¿å­˜å»¶è¿Ÿã€‚
- **ç¼“å­˜**: æ¯ä¸ªçº¿ç¨‹å—å¤„ç†ä¸€ä¸ªtileå‰å…ˆå°†å…¶ä»Žå…¨å±€å†…å­˜ç¼“å­˜åˆ°å…±äº«å†…å­˜ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªfragå‰å°†å…¶ä»Žå…±äº«å†…å­˜ç¼“å­˜åˆ°å¯„å­˜å™¨ä¸­ã€‚å‡å°‘è®¿å­˜æ¬¡æ•°ï¼Œé˜²æ­¢è®¡ç®—æ ¸å¿ƒç­‰å¾…ï¼Œæé«˜å…¶åˆ©ç”¨çŽ‡ã€‚
- **é¢„å–**: åœ¨å¤„ç†å½“å‰tile/fragæ—¶ï¼Œå¯ä»¥å‘å°„æŒ‡ä»¤é¢„åŠ è½½ä¸‹ä¸€ä¸ªtile/fragåˆ°å¯„å­˜å™¨ä¸­ã€‚å½“å‰è®¡ç®—ä¸ä¾èµ–è¿™æ¬¡è®¿å­˜ï¼Œå¯å¹¶è¡Œæ‰§è¡Œï¼Œå®žçŽ°ä¸€å®šçš„è®¿å­˜å»¶è¿Ÿéšè—

### M=N=K, M æ•´é™¤64çš„æƒ…å†µ
RTX 3060:
![RTX3060 SgemmDiv64æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_3060_div64.png)
RTX 4090:
![RTX4090 SgemmDiv64æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_4090_div64.png)

### M=N=K, M æ•´é™¤1çš„æƒ…å†µ
è¿™ç§æƒ…å†µç›®å‰åªæ˜¯åœ¨ div64 çš„åŸºç¡€ä¸Šå°†æ‰€æœ‰ float4 å­˜å–å±•å¼€ï¼Œå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼ŒåŽç»­ä¼˜åŒ–TODO
RTX 3060:
![RTX3060 SgemmDiv1æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_3060_div1.png)
RTX 4090:
![RTX4090 SgemmDiv1æ€§èƒ½å¯¹æ¯”å›¾](https://github.com/BowTen/mLLM/raw/main/resources/gemm_performance_comparison_4090_div1.png)


## ðŸ¤– Qwen3 æž¶æž„å›¾
![Qwen3æž¶æž„å›¾](https://github.com/BowTen/mLLM/raw/main/resources/qwen3_arc.png)

## âŒ¨ï¸ èŠå¤©ç¤ºä¾‹

```cpp
#include "model/qwen3.h"

using namespace mllm;
using namespace mllm::tokenizer;

class Qwen3Chat
{
public:
    Qwen3Chat(const std::string &model_path, mllm::base::Device device, float temperature)
        : model(mllm::model::Qwen3::from_pretrained(model_path, device, temperature)),
          tokenizer(model.get_tokenizer())
    {
    }

    std::string chat(const std::string &input)
    {
        auto chat_token_ids = tokenizer->encode_with_chat_template(input, true, true);
        auto input_id = tokenizer->to_tensor(chat_token_ids, model.device());
        base::Tensor next_id({1, 1}, model.device(), false, model.stream());

        std::string output;
        while (true)
        {
            input_id.toDevice(model.device());
            next_id.toDevice(model.device());
            model.forward(input_id, next_id);
            next_id.toDevice(base::Device::CPU);
            uint32_t id = *(reinterpret_cast<uint32_t *>(next_id.data()));

            std::string next_token = tokenizer->decode(id);
            output.append(next_token);

            if (id == BPETokenizer::QWEN3_END_OF_TEXT || id == BPETokenizer::QWEN3_IM_END)
            {
                std::cout << std::endl;
                break;
            }
            std::cout << next_token;
            std::cout.flush();
            input_id = next_id.clone();
        }

        return output;
    }

private:
    mllm::model::Qwen3 model;
    BPETokenizerPtr tokenizer;
};

int main()
{
    std::string model_path = "path_to_your_resources/Qwen/Qwen3-0.6B";
    std::cout << "Loading model..." << std::endl;
    Qwen3Chat qwen3(model_path, base::Device::CUDA, 0.6);
    std::cout << "Loading accomplished." << std::endl;

    while (true)
    {
        std::string input;
        std::cout << "User: ";
        std::cin >> input;
        if (input == "exit")
            break;
        std::cout << "Qwen3: ";
        std::cout.flush();
        qwen3.chat(input);
    }

    return 0;
}
```

![Qwen3 chat demo æ¼”ç¤ºåŠ¨å›¾](https://github.com/BowTen/mLLM/raw/main/resources/chat_demo.gif)


## ä¾èµ–åº“

- armadillo
- glog
- gtest
- cnpy